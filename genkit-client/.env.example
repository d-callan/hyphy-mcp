# Model configuration
MODEL_NAME=gemini-2.5-flash
MODEL_TEMPERATURE=0.7

# Model provider selection
# Options: google, openai, anthropic, ollama, deepseek, local
MODEL_PROVIDER=google

# API Keys for different providers
GOOGLE_API_KEY=your_google_api_key_here
# OPENAI_API_KEY=your_openai_api_key_here
# ANTHROPIC_API_KEY=your_anthropic_api_key_here
# DEEPSEEK_API_KEY=your_deepseek_api_key_here

# Ollama configuration
# OLLAMA_URL=http://localhost:11434

# Available models by provider:
# Google: gemini-2.5-flash, gemini-2.5-pro, gemini-1.5-pro, gemini-1.5-flash
# OpenAI: gpt-4o, gpt-4-turbo, gpt-3.5-turbo
# Anthropic: claude-3-5-sonnet, claude-3-5-haiku, claude-3-7-sonnet, claude-3-haiku, claude-3-opus, claude-3-sonnet, claude-4-opus, claude-4-sonnet
# Ollama: depends on your local Ollama setup (e.g., llama3, mistral, etc.)
# DeepSeek: deepseek-chat, deepseek-coder

# Server configuration
# SERVER_PORT=3000
# DEBUG_MODE=true
